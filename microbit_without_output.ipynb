{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Micro:bit project\n",
    "#Project description\n",
    "#The company \"Strawbees\" for which this project was carried out sells STEAM learning materials that are compatible with the micro:bit device.\n",
    "\n",
    "#The comlany collabs with Micro:bit Educational Foundation, so is able to get a data from them about their purchases as part of the exchange.\n",
    "#Strawbees uses a CRM system (Hubspot) to keep track of US school districts, schools, educational resellers, e.t.c. they maintain a relationship with.\n",
    "#A Micro:bit dataset contains a list of US schools districts that had spent part of their budget on acquiring micro:bit devices. These schools districts are of high interest for establishing communications for Strawbees. However, in order to identify these districts within the company's CRM, the records from the Microbit dataset need to be linked to the CRM records. \n",
    "#The project involves matching these two datasets through exact and fuzzy linking techniques.\n",
    "\n",
    "# Micro:bit project outline:\n",
    "\n",
    "#CSV_files for the project:\n",
    "#1. Micro:bit school districts file from collaborator - number of district to match - 1897.\n",
    "#2. US school districts file from NCES - official source of National Center for Education Statistics \n",
    "#https://nces.ed.gov/ccd/files.asp#Fiscal:2,LevelId:5,SchoolYearId:38,Page:1 - file location.\n",
    "#Total number of US school districts - 19627.\n",
    "\n",
    "#Part 1: Preparing micro:bit district file for the import.\n",
    "\n",
    "#Part 2: Preparing US school districts import file.\n",
    "\n",
    "#Part 3: Matching through exact name.\n",
    "\n",
    "#Part 4: Record linkage through similar names. \n",
    "\n",
    "# Imports required for the project\n",
    "#Importing Pandas library for data manipulation and Fuzz (a string matching library) for fuzzy string matching operations.\n",
    "#Fuzzy string matching refers to finding strings that are approximately rather than exactly equal, allowing for minor differences like typos or variations.\n",
    "\n",
    "import pandas as pd\n",
    "from thefuzz import fuzz, process\n",
    "\n",
    "# Exploring and cleaning data\n",
    "\n",
    "data_nces = pd.read_csv(\"school_districts_NCES.csv\",dtype={'LEAID':str})\n",
    "\n",
    "data_microbit = pd.read_csv(\"school_districts_micro_bit.csv\")\n",
    "\n",
    "data_nces.head(2)\n",
    "\n",
    "data_microbit.head(2)\n",
    "\n",
    "#Number of districts to match\n",
    "data_microbit.shape[0]\n",
    "\n",
    "#Part 1. Preparing micro:bit district file for the import\n",
    "#Exploring Row partner's data requirued for the project, filter out unneeded, empty data. \n",
    "#(According NDA data in columns 'Total spend (approx)' and 'Number of purchases' are  hidden)\n",
    "#Spliting 'Agency' column into 'District Name' and 'State'\n",
    "\n",
    "data_microbit.drop(labels=[\"Unnamed: 1\", \"Number of purchases\"], axis=1, inplace=True)\n",
    "data_microbit.head(2)\n",
    "\n",
    "data_microbit[['district','state']] = data_microbit[\"Agency\"].str.split(pat=\", \", n=1, expand=True)\n",
    "data_microbit.head(2)\n",
    "\n",
    "#Part 2: Preparing US school districts import file.\n",
    "\n",
    "#Filtering out unneeded columns (keep at least STATENAME, LEAID, LEA_NAME)\n",
    "#Important: Importing LEAID as string rather than integer. Using dtype parameter from pandas.read_csv (using this source for the project and skills developing).\n",
    "#Adding prefix (NCES_District-) to LEAID in order to create National Data ID column (similar with the Property name in Hubspot SRM Strawbees DataBase).\n",
    "#Capitalising DISTRICTNAME, STATENAME columns (other if needed) to match the values in micro.bit file.\n",
    "#Capitalising all distcricts names in both files for more accurate matches.\n",
    "\n",
    "data_nces.columns\n",
    "\n",
    "labels_to_drop = [\"SCHOOL_YEAR\", \"FIPST\", \"ST\", \"STATE_AGENCY_NO\", \"UNION\", \"ST_LEAID\", \"MSTREET2\", \"MSTREET3\", \"MSTATE\", \"MZIP\", \"MZIP4\", \"LSTREET1\", \"LSTREET2\", \"LSTREET3\", \n",
    "\"LCITY\", \"LSTATE\", \"LZIP\", \"LZIP4\", \"PHONE\", \"SY_STATUS\", \"SY_STATUS_TEXT\", \"UPDATED_STATUS\", \"UPDATED_STATUS_TEXT\",\n",
    "\"EFFECTIVE_DATE\", \"LEA_TYPE\", \"LEA_TYPE_TEXT\", \"OUT_OF_STATE_FLAG\", \"CHARTER_LEA\", \"CHARTER_LEA_TEXT\", \"NOGRADES\", \"G_PK_OFFERED\",\n",
    "\"G_KG_OFFERED\", \"G_1_OFFERED\", \"G_2_OFFERED\", \"G_3_OFFERED\", \"G_4_OFFERED\", \"G_5_OFFERED\", \"G_6_OFFERED\", \"G_7_OFFERED\",\n",
    "\"G_8_OFFERED\", \"G_9_OFFERED\", \"G_10_OFFERED\", \"G_11_OFFERED\", \"G_12_OFFERED\", \"G_13_OFFERED\", \"G_UG_OFFERED\", \"G_AE_OFFERED\", \"GSLO\",\n",
    "\"GSHI\", \"LEVEL\", \"IGOFFERED\", \"OPERATIONAL_SCHOOLS\"]\n",
    "\n",
    "data_nces.drop(labels=labels_to_drop, axis=1, inplace=True)\n",
    "data_nces.head(2)\n",
    "\n",
    "prefix = 'NCES_District-'\n",
    "data_nces['LEAID'] = prefix + data_nces['LEAID']\n",
    "data_nces.head(2)\n",
    "\n",
    "mapping = {'LEAID': 'NATIONAL DATA ID', 'LEA_NAME': 'DISTRICT_NAME', 'MSTREET1': 'STREET_ADDRESS', 'MCITY': 'CITY', \n",
    "'WEBSITE': 'COMPANY_DOMAIN'}\n",
    "data_nces = data_nces.rename(columns=mapping)\n",
    "data_nces.head(2)\n",
    "\n",
    "total_districts = len(data_nces)\n",
    "print(f\"Number of total districts in the dataframe: {total_districts}\")\n",
    "\n",
    "data_nces['DISTRICT_NAME'] = data_nces['DISTRICT_NAME'].str.upper()\n",
    "data_nces.head(3)\n",
    "\n",
    "# Joining data_microbit with data_nces dataframes.\n",
    "\n",
    "#Part 3: Matching through exact name.\n",
    "\n",
    "#Renaming columns in the right order.\n",
    "#Merging/joining micro:bit dataframe with NCES dataframe through exact District name and State name \n",
    "#(Merge type (how) affects the results - I use 'Left' in order to keep all rows from the left dataframe (data_microbit), and only matching rows from the right (data_nces).)\n",
    "#Output: micro:bit dataframe has a column with National Data ID, which is empty for some districts (those without exact match) and filled for those with exact.\n",
    "#Spliting resulting dataframe into those rows with National data ID - to be imported; and the ones without National data ID - be able to work on Part 4 of the project.\n",
    "\n",
    "data_microbit = data_microbit.rename(columns={'district': 'DISTRICT_NAME', 'state': 'STATENAME'})\n",
    "data_microbit.head(3)\n",
    "\n",
    "data_microbit['STATENAME'] = data_microbit['STATENAME'].str.upper()\n",
    "data_microbit.head(3)\n",
    "\n",
    "data_microbit['DISTRICT_NAME'] = data_microbit['DISTRICT_NAME'].str.upper()\n",
    "data_microbit.head(3)\n",
    "\n",
    "data_microbit.dtypes\n",
    "\n",
    "#merging dataframes\n",
    "merged_data_microbit_nces = pd.merge(data_microbit, data_nces, \n",
    "                     left_on=['DISTRICT_NAME', 'STATENAME'], \n",
    "                     right_on=['DISTRICT_NAME', 'STATENAME'],\n",
    "                     how='left') \n",
    "\n",
    "merged_data_microbit_nces.head(10)\n",
    "\n",
    "total_districts = len(merged_data_microbit_nces)\n",
    "print(f\"Number of total districts in the merged dataframe: {total_districts}\")\n",
    "\n",
    "#sampling dataframes with matched and non-matched school districts\n",
    "nationaldataid_present = merged_data_microbit_nces[merged_data_microbit_nces['NATIONAL DATA ID'].notna()]\n",
    "nationaldataid_missing = merged_data_microbit_nces[merged_data_microbit_nces['NATIONAL DATA ID'].isna()]\n",
    "\n",
    "nationaldataid_present.shape[0]\n",
    "\n",
    "nationaldataid_missing.shape[0]\n",
    "\n",
    "# Record linkage / fuzzy matching\n",
    "#For fuzzy matching - Skills based on course \"Cleaning data with pandas\" in DataCamp.\n",
    "nationaldataid_present.to_csv('nationaldataid_present.csv', index=False)\n",
    "nationaldataid_missing.to_csv('nationaldataid_missing.csv', index=False)\n",
    "\n",
    "# Fuzzy matching code\n",
    "# Initialize needed lists to store results\n",
    "match_count = 0\n",
    "matched_ids = []\n",
    "street = []\n",
    "city = []\n",
    "domain = []\n",
    "names = []\n",
    "# Iterate through each row in nationaldataid_missing\n",
    "for index, row in nationaldataid_missing.iterrows():\n",
    "    district_to_match = row['DISTRICT_NAME']\n",
    "    state = row['STATENAME']\n",
    "    # Convert values to strings \n",
    "    district_to_match = str(district_to_match) if not pd.isna(district_to_match) else 'Unknown'\n",
    "    state = str(state) if not pd.isna(state) else 'Unknown'\n",
    "    #print(f\"Matching for: {district_to_match}: {state}\")\n",
    "    data_nces['Match_Score'] = data_nces['DISTRICT_NAME'].apply(lambda x: fuzz.WRatio(district_to_match, str(x)))\n",
    "    # Filter rows by state and sort by match score\n",
    "    best_match_row = data_nces[(data_nces['STATENAME'] == state)].sort_values('Match_Score', ascending=False).reset_index()\n",
    "    \n",
    "    # How we check if we have a matching row\n",
    "    if not best_match_row.empty:\n",
    "        best_match_name = best_match_row.loc[0, 'DISTRICT_NAME']\n",
    "        national_id = best_match_row.loc[0, 'NATIONAL DATA ID']\n",
    "        match_street = best_match_row.loc[0, 'STREET_ADDRESS']\n",
    "        match_city = best_match_row.loc[0, 'CITY']\n",
    "        matched_domain = best_match_row.loc[0, 'COMPANY_DOMAIN']\n",
    "        #print(f\"Best match: {best_match_name} with National ID: {national_id}\")\n",
    "        matched_ids.append(national_id)\n",
    "        street.append(match_street)\n",
    "        city.append(match_city)\n",
    "        domain.append(matched_domain)\n",
    "        names.append(best_match_name)\n",
    "        match_count += 1  # Increment match count\n",
    "    else:\n",
    "        #print(\"No match found.\")\n",
    "        matched_ids.append(None)\n",
    "        names.append(None)\n",
    "        street.append(None)\n",
    "        city.append(None)\n",
    "        domain.append(None)\n",
    "# Add the matched National IDs to nationaldataid_missing data_frame\n",
    "nationaldataid_missing['NATIONAL DATA ID'] = matched_ids\n",
    "nationaldataid_missing['matched_name'] = names\n",
    "nationaldataid_missing['STREET_ADDRESS'] = street\n",
    "nationaldataid_missing['CITY'] = city\n",
    "nationaldataid_missing['COMPANY_DOMAIN'] = domain\n",
    "print(f\"Total number of matches: {match_count}\")\n",
    "# Filter rows by district name and sort by match score to get the best match\n",
    "data_nces['Match_Score'] = data_nces['DISTRICT_NAME'].apply(lambda x: fuzz.WRatio('TRACY UNIFIED SCHOOL DISTRICT', str(x)))\n",
    "best_match_row = data_nces[(data_nces['STATENAME'] == state)].sort_values('Match_Score', ascending=False).reset_index()\n",
    "# Check how does the funcrion works\n",
    "best_match_row.loc[0,'DISTRICT_NAME']\n",
    "# There's 3 states that had commas in their statename and were incorrectly matched\n",
    "# Keep them for manual correction in the final csv.file before doing import sorted and organised data into the Strawbees Hubspot CRM DataBase.\n",
    "nationaldataid_missing[nationaldataid_missing['NATIONAL DATA ID'].isna()]\n",
    "# Check the number of all metched districts\n",
    "print(f\"Total number of matches: {match_count}\")\n",
    "\n",
    "# Joining data frames nationaldataid_missing and nationaldataid_present\n",
    "\n",
    "#Part 5: Merging resulting dataframes for final version of school districts with all needed data ready for the import in Strawbees Hubspot CRM Database.\n",
    "\n",
    "# Preparing final dataset for the import micro:bit data:\n",
    "# 1. Combining both datasets\n",
    "# 2. Adding \"use_microbit\" column with value \"Yes\"\n",
    "# 3. Exporting to spreadsheet for manual correction\n",
    "\n",
    "# Delete no needed columns\n",
    "nationaldataid_missing.drop(labels=[\"matched_name\"], axis=1, inplace=True)\n",
    "nationaldataid_missing.head(2)\n",
    "\n",
    "# Show total number of values after fuzzy matching\n",
    "nationaldataid_missing.shape[0]\n",
    "\n",
    "nationaldataid_present.head()\n",
    "\n",
    "# Combining both datasets after data exploring, manipulation and merging\n",
    "microbitdata_for_import = pd.concat([nationaldataid_missing, nationaldataid_present], ignore_index=True)\n",
    "microbitdata_for_import.head()\n",
    "\n",
    "total_districts = len(microbitdata_for_import)\n",
    "\n",
    "#Adding coulmn \"use_microbit\" (similar with the Property name in Hubspot SRM Strawbees DataBase).\n",
    "\n",
    "# Adding \"use_microbit\" column with value \"Yes\" \n",
    "microbitdata_for_import[\"use_microbit\"] = \"Yes\"\n",
    "microbitdata_for_import.head()\n",
    "\n",
    "# Exporting final resulting dataframe to spreadsheet for manual correction\n",
    "microbitdata_for_import.to_csv('microbitdata_for_import.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
